考察Push-Diging 的算法
他的$\frac{1}{n}\mathbb{1}_{n}^T\nabla \mathbf{f}(\mathbf{x}^k)=\frac{1}{n}\sum_{i=1}^n\nabla f_{i}(x_{i}^k)$
是由于consensus的保证（使用的push-sum的策略），才可以让所有的$x_{i}$趋于一致，从而$\frac{1}{n}\sum_{i=1}^n\nabla f_{i}(x_{i}^k)=\frac{1}{n}\sum_{i=1}^n\nabla f_{i}(x_{*}^k)\to {0}$

然而push-pull里面，可以先用数值实验看一看（一定是consensus趋于一致），那么就是要找怎么去证明这个consensus，因为我们之前不是说，最后的平衡态是一个加权平均的结果吗。。。

+ （LLY）push-diging中是使用push-sum来做consensus,数值上看push-sum consensus 的效率很低，cs_error很容易爆炸的。经常是cs_error爆炸导致grad_norm爆炸。push-pull是通过乘一个行随机来做consensus,数值上看cs_error非常平稳，很难爆炸的。但是grad_norm下降的速度似乎受到data heterogeneity影响很大

+ The row-stochasticity of the weight matrix guarantees that all agents reach consensus, while the column-stochasticity ensures that each agents local gradient contributes equally to the global objective.


$$
\mathbf{g}=\begin{bmatrix}
g_{1} \\
\vdots \\
g_{n}
\end{bmatrix}, g_{i} \in \mathbb{R}^{1*d}, \mathbf{g} \in \mathbb{R}^{n*d}
$$
怎么求
$$
\begin{bmatrix}
\sum_{i=1}^n g_{i} \\
\vdots \\
\sum_{i=1}^n g_{i}
\end{bmatrix}
$$

$||  \mathbf{x}^k-\frac{1}{n}\mathbb{1}_{n}\mathbb{1}_{n}^T\mathbf{x}^k ||-||   \mathbf{x}^k-\mathbf{w}^k||$
